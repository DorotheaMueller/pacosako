
function model_from_artifact(name; kwargs...)
  path = try
    artifact = "model.$name"
    @artifact_str artifact
  catch
    error("No model artifact with name $name exists")
  end
  Model.load(joinpath(path, "$name.jtm"); kwargs...)
end


"""
    Ludwig(version = "1.0"; kwargs...)

Return a pretrained model with the ludwig architecture. `all` keyword arguments
are passed to `Model.configure`.

On first usage, the model is downloaded via the artifact system.

# Architecture
The ludwig architecture is an Alpha-Zero inspired resnet with `6` residual
blocks and `128` filters.

See [`Jtac.Model.Zoo.ZeroRes`](@ref).

# Observations
Ludwig is strong in immediate endgame positions, and can attack / defend well
if the paco depth is not high. The opening and midgame strength of ludwig seems
to be very limited.

Several attempts were made to improve upon version `1.0`. So far, only the
human finetuning seems to have brought improvement (by improving the opening).
When running play-train loops, the policy loss would converge but the
value loss would remain sizable. This, together with further observations about
the performance of models with additional training on top of version `1.0`,
indicate that the model architecture is not sufficient to understand the value
labels of non-endgame positions.

# Versions
**`1.0`**

Trained from scratch on several million game states produced by a Luna based
player.

* Base: `ZeroRes(blocks = 6, filters = 128)`
* Player: `MCTSPlayer(Luna(), power = 10000)`
* Optimizer: mostly `Momentum(1e-2)`, then fine tuning via `Momentum(1e-3)`

**`1.0-human`**

Human aligned version of Ludwig. Version `"1.0"` was fine-tuned for 10
epochs on (a part of) the pacoplay database to favor behavior (especially in
openings) that are more human-like.

* Base: `Ludwig("1.0")`
* Player: humans (pacoplay database)
* Optimizer: `Momentum(1e-3)`
"""
function Ludwig(version = "1.0"; kwargs...)
  model_from_artifact("ludwig-$version"; kwargs...)
end


"""
    Hedwig(version = "0.2"; kwargs...)

Return the pretrained hedwig model. All keyword arguments are passed to
`Model.configure`.

On first usage, the model is downloaded via the artifact system.

# Architecture
The hedwig architecture is an Alpha-Zero inspired resnet with `12` residual
blocks and `256` filters.

See [`Jtac.Model.Zoo.ZeroRes`](@ref).

# Versions

**`0.1`**

Preliminary version of Hedwig. Trained from scratch on about 11 million game
states produced by selfplays of a Ludwig based player.

* Base: `ZeroRes(blocks = 12, filters = 256)`
* Player: `MCTSPlayerGumbel(Ludwig("1.0"), power = 1000)`
* Optimizer: `Adam(1e-3)`.

**`0.2`**

Self-improvement of version `0.1` for 60 play-train generations. Each generation
consisted of ~500k game states generated by a gumbel player.

* Base: `Hedwig("0.1")`
* Player: `MCTSPlayerGumbel(model, power = 64)`
* Optimizer: `Adam(1e-3)` (first 55 generations), \
             `Adam(1e-4)` (final 5 generations)


**`0.3`**

Self-improvement of version `0.2` for 45 play-train generations. Each generation
consisted of 150k to 200k game states generated by a gumbel player.

* Base: `Hedwig("0.2")`
* Player: `MCTSPlayerGumbel(model, power = 64)`
* Optimizer: `Adam(1e-4)`


**`0.4`**

Self-improvement of version `0.3` for 40 play-train generations. Each generation
consisted of 150k to 200k game states generated by a gumbel player.

* Base: `Hedwig("0.3")`
* Player: `MCTSPlayerGumbel(model, power = 64)`
* Optimizer: `Adam(1e-4)`


**`0.5`**

Self-improvement of version `0.4` for 15 play-train generations. Each generation
consisted of 150k to 200k game states generated by a gumbel player.

* Base: `Hedwig("0.4")`
* Player: `MCTSPlayerGumbel(model, power = 64)`
* Optimizer: `Adam(1e-4)`

**`0.6`**

Self-improvement of version `0.5` for 50 play-train generations. Each generation
consisted of 200k to 300k game states generated by a gumbel player.

* Base: `Hedwig("0.5")`
* Player: `MCTSPlayerGumbel(model, power = 64)`
* Optimizer: `Adam(1e-4)`

**`0.7`**

Self-improvement of version `0.6` for 170 play-train generations. Each generation
consisted of 200k to 300k game states generated by a gumbel player.

* Base: `Hedwig("0.6")`
* Player: `MCTSPlayerGumbel(model, power = 64)`
* Optimizer: `Adam(1e-4)`

**`0.8`**

Self-improvement of version `0.7` for about 300 play-train generations.
Each generation consisted of 200k to 300k game states generated by a gumbel 128
player. A "best" model out of several candidates was picked.

* Base: `Hedwig("0.7")`
* Player: `MCTSPlayerGumbel(model, power = 128)`
* Optimizer: `Adam(1e-4)`
"""
function Hedwig(version = "0.8"; kwargs...)
  model_from_artifact("hedwig-$version"; kwargs...)
end
